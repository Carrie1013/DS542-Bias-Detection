{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file_path = \"fairface-img-margin025-trainval.zip\"\n",
    "extract_to_dir = \"data\"\n",
    "\n",
    "os.makedirs(extract_to_dir, exist_ok=True)\n",
    "\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "age_prompts = ['3-9', '50-59', '30-39', '20-29', 'more than 70', '40-49', '10-19', '60-69', '0-2']\n",
    "gender_prompts = ['Male', 'Female']\n",
    "race_prompts = ['East Asian', 'White', 'Latino_Hispanic', 'Southeast Asian', 'Black', 'Indian', 'Middle Eastern']\n",
    "\n",
    "csv_file_path = \"data/fairface_label_val.csv\"\n",
    "data = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairfaceDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transforms, target_cols):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.target_cols = target_cols\n",
    "        \n",
    "        # Filter valid rows with existing image files\n",
    "        self.valid_data = self.data[self.data['file'].apply(\n",
    "            lambda x: os.path.exists(os.path.join(self.image_dir, x))\n",
    "        )]\n",
    "        self.missing_count = len(self.data) - len(self.valid_data)\n",
    "        if self.missing_count > 0:\n",
    "            print(f\"Warning: {self.missing_count} files are missing and will be skipped.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.valid_data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, row['file'])\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transforms(image)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "        targets = {col: row[col] for col in self.target_cols}\n",
    "        return image, targets\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None  \n",
    "    images, targets = zip(*batch)  \n",
    "    return torch.stack(images), {key: [d[key] for d in targets] for key in targets[0]}\n",
    "\n",
    "def fine_tune_clip(train_csv, val_csv, image_dir, output_dir, epochs=20, batch_size=15, lr=1e-4):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Running on {device}\")\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model = model.float()\n",
    "    \n",
    "    for param in model.visual.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    num_classes = {\n",
    "        \"age\": len(age_prompts),\n",
    "        \"gender\": len(gender_prompts),\n",
    "        \"race\": len(race_prompts)\n",
    "    }\n",
    "    classification_heads = {\n",
    "        col: nn.Linear(model.visual.output_dim, num_classes[col]).to(device)\n",
    "        for col in num_classes\n",
    "    }\n",
    "\n",
    "    # Optimizer\n",
    "    params = [p for head in classification_heads.values() for p in head.parameters()]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = FairfaceDataset(train_csv, image_dir, preprocess, target_cols=[\"age\", \"gender\", \"race\"])\n",
    "    val_dataset = FairfaceDataset(val_csv, image_dir, preprocess, target_cols=[\"age\", \"gender\", \"race\"])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            losses = []\n",
    "            for col, head in classification_heads.items():\n",
    "                labels = torch.tensor([age_prompts.index(label) if col == \"age\" else\n",
    "                               gender_prompts.index(label) if col == \"gender\" else\n",
    "                               race_prompts.index(label)\n",
    "                               for label in targets[col]]).to(device)\n",
    "                head = head\n",
    "                logits = head(model.visual(images))\n",
    "                loss = criterion(logits, labels)\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Backpropagation\n",
    "            total_loss = sum(losses)\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_accuracies = {col: [] for col in classification_heads}\n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "                images = images.to(device)\n",
    "                for col, head in classification_heads.items():\n",
    "                    labels = torch.tensor([age_prompts.index(label) if col == \"age\" else\n",
    "                               gender_prompts.index(label) if col == \"gender\" else\n",
    "                               race_prompts.index(label)\n",
    "                               for label in targets[col]]).to(device)\n",
    "\n",
    "                    head = head\n",
    "                    logits = head(model.visual(images))\n",
    "                    preds = logits.argmax(dim=-1)\n",
    "                    accuracy = accuracy_score(labels.cpu(), preds.cpu())\n",
    "                    val_accuracies[col].append(accuracy)\n",
    "\n",
    "        # Validation Result\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for col, accuracies in val_accuracies.items():\n",
    "            print(f\"{col} accuracy: {sum(accuracies)/len(accuracies):.2%}\")\n",
    "\n",
    "    # Save fine-tuned model\n",
    "    for col, head in classification_heads.items():\n",
    "        torch.save(head.state_dict(), os.path.join(output_dir, f\"{col}_head.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_csv = \"data/fairface_label_train.csv\"\n",
    "val_csv = \"data/fairface_label_val.csv\"\n",
    "image_dir = \"data\"\n",
    "output_dir = \"data/fine_tuned_clip\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "fine_tune_clip(train_csv, val_csv, image_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1377545.1.academic-gpu/ipykernel_3401547/258261346.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  head.load_state_dict(torch.load(os.path.join(output_dir, f\"{col}_head.pth\")))\n",
      "Validating: 100%|██████████| 343/343 [00:45<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "age accuracy: 59.62%\n",
      "gender accuracy: 94.70%\n",
      "race accuracy: 71.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_clip(val_csv, image_dir, output_dir, batch_size=32):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Running on {device}\")\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model = model.float()\n",
    "\n",
    "    for param in model.visual.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    classification_heads = {\n",
    "        \"age\": nn.Linear(model.visual.output_dim, len(age_prompts)).to(device),\n",
    "        \"gender\": nn.Linear(model.visual.output_dim, len(gender_prompts)).to(device),\n",
    "        \"race\": nn.Linear(model.visual.output_dim, len(race_prompts)).to(device),\n",
    "    }\n",
    "\n",
    "    for col, head in classification_heads.items():\n",
    "        head.load_state_dict(torch.load(os.path.join(output_dir, f\"{col}_head.pth\")))\n",
    "        head.eval()\n",
    "\n",
    "    val_dataset = FairfaceDataset(val_csv, image_dir, preprocess, target_cols=[\"age\", \"gender\", \"race\"])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    model.eval()\n",
    "    val_accuracies = {col: [] for col in classification_heads}\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "            if images is None or targets is None:\n",
    "                continue\n",
    "            images = images.to(device)\n",
    "            for col, head in classification_heads.items():\n",
    "                labels = torch.tensor(\n",
    "                    [\n",
    "                        age_prompts.index(label) if col == \"age\" else\n",
    "                        gender_prompts.index(label) if col == \"gender\" else\n",
    "                        race_prompts.index(label)\n",
    "                        for label in targets[col]\n",
    "                    ]\n",
    "                ).to(device)\n",
    "\n",
    "                logits = head(model.visual(images))\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                accuracy = accuracy_score(labels.cpu(), preds.cpu())\n",
    "                val_accuracies[col].append(accuracy)\n",
    "\n",
    "    print(\"Validation Results:\")\n",
    "    for col, accuracies in val_accuracies.items():\n",
    "        if accuracies:  \n",
    "            print(f\"{col} accuracy: {sum(accuracies) / len(accuracies):.2%}\")\n",
    "        else:\n",
    "            print(f\"{col}: No valid data to validate.\")\n",
    "\n",
    "validate_clip( val_csv, image_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cq1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
